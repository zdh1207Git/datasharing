How to share data with a statistician
===========

This is a guide for anyone who needs to share data with a statistician or data scientist. The target audiences I have in mind are:

* Collaborators who need statisticians or data scientists to analyze data for them
* Students or postdocs in various disciplines looking for consulting advice
* Junior statistics students whose job it is to collate/clean/wrangle data sets

The goals of this guide are to provide some instruction on the best way to share data to avoid the most common pitfalls
and sources of delay in the transition from data collection to data analysis. The [Leek group](http://biostat.jhsph.edu/~jleek/) works with a large
number of collaborators and the number one source of variation in the speed to results is the status of the data
when they arrive at the Leek group. Based on my conversations with other statisticians this is true nearly universally.

My strong feeling is that statisticians should be able to handle the data in whatever state they arrive. It is important
to see the raw data, understand the steps in the processing pipeline, and be able to incorporate hidden sources of
variability in one's data analysis. On the other hand, for many data types, the processing steps are well documented
and standardized. So the work of converting the data from raw form to directly analyzable form can be performed 
before calling on a statistician. This can dramatically speed the turnaround time, since the statistician doesn't
have to work through all the pre-processing steps first. 

如何与统计学家共享数据
============

这是任何需要与统计学家或数据科学家共享数据的人的指南。我心目中的目标受众是：

* 需要统计学家或数据科学家为他们分析数据的合作者
* 各个学科的学生或博士后寻求咨询建议
* 初级统计学学生，其工作是整理/清理/整理数据集

本指南的目标是提供一些有关共享数据的最佳方式的说明，以避免最常见的陷阱
从数据收集到数据分析的过渡延迟的来源。 [Leek group](http://biostat.jhsph.edu/~jleek/) 与一个大
合作者的数量和结果速度变化的第一个来源是数据的状态
当他们到达韭菜集团时。根据我与其他统计学家的谈话，这几乎是普遍适用的。

我强烈的感觉是统计学家应该能够处理他们到达的任何状态的数据。这很重要
查看原始数据，了解处理流程中的步骤，并能够合并隐藏的数据源
数据分析的可变性。另一方面，对于许多数据类型，处理步骤都有详细记录
和标准化。因此可以执行将数据从原始形式转换为可直接分析形式的工作
在请教统计学家之前。这可以显着加快周转时间，因为统计学家没有
必须首先完成所有预处理步骤。
What you should deliver to the statistician
====================

To facilitate the most efficient and timely analysis this is the information you should pass to a statistician:

1. The raw data.
2. A [tidy data set](http://vita.had.co.nz/papers/tidy-data.pdf) 
3. A code book describing each variable and its values in the tidy data set.  
4. An explicit and exact recipe you used to go from 1 -> 2,3 

Let's look at each part of the data package you will transfer. 


### The raw data

It is critical that you include the rawest form of the data that you have access to. This ensures
that data provenance can be maintained throughout the workflow.  Here are some examples of the
raw form of data:

* The strange [binary file](http://en.wikipedia.org/wiki/Binary_file) your measurement machine spits out
* The unformatted Excel file with 10 worksheets the company you contracted with sent you
* The complicated [JSON](http://en.wikipedia.org/wiki/JSON) data you got from scraping the [Twitter API](https://twitter.com/twitterapi)
* The hand-entered numbers you collected looking through a microscope

You know the raw data are in the right format if you: 

1. Ran no software on the data
1. Did not modify any of the data values
1. You did not remove any data from the data set
1. You did not summarize the data in any way

If you made any modifications of the raw data it is not the raw form of the data. Reporting modified data
as raw data is a very common way to slow down the analysis process, since the analyst will often have to do a
forensic study of your data to figure out why the raw data looks weird. (Also imagine what would happen if new data arrived?)

### The tidy data set

The general principles of tidy data are laid out by [Hadley Wickham](http://had.co.nz/) in [this paper](http://vita.had.co.nz/papers/tidy-data.pdf)
and [this video](http://vimeo.com/33727555). While both the paper and the video describe tidy data using [R](http://www.r-project.org/), the principles
are more generally applicable:

1. Each variable you measure should be in one column
1. Each different observation of that variable should be in a different row
1. There should be one table for each "kind" of variable
1. If you have multiple tables, they should include a column in the table that allows them to be joined or merged

While these are the hard and fast rules, there are a number of other things that will make your data set much easier
to handle. First is to include a row at the top of each data table/spreadsheet that contains full row names. 
So if you measured age at diagnosis for patients, you would head that column with the name `AgeAtDiagnosis` instead
of something like `ADx` or another abbreviation that may be hard for another person to understand. 


Here is an example of how this would work from genomics. Suppose that for 20 people you have collected gene expression measurements with 
[RNA-sequencing](http://en.wikipedia.org/wiki/RNA-Seq). You have also collected demographic and clinical information
about the patients including their age, treatment, and diagnosis. You would have one table/spreadsheet that contains the clinical/demographic
information. It would have four columns (patient id, age, treatment, diagnosis) and 21 rows (a row with variable names, then one row
for every patient). You would also have one spreadsheet for the summarized genomic data. Usually this type of data
is summarized at the level of the number of counts per exon. Suppose you have 100,000 exons, then you would have a
table/spreadsheet that had 21 rows (a row for gene names, and one row for each patient) and 100,001 columns (one row for patient
ids and one row for each data type). 

If you are sharing your data with the collaborator in Excel, the tidy data should be in one Excel file per table. They
should not have multiple worksheets, no macros should be applied to the data, and no columns/cells should be highlighted. 
Alternatively share the data in a [CSV](http://en.wikipedia.org/wiki/Comma-separated_values) or [TAB-delimited](http://en.wikipedia.org/wiki/Tab-separated_values) text file. (Beware however that reading CSV files into Excel can sometimes lead to non-reproducible handling of date and time variables.)

你应该向统计学家提供什么
====================

为了促进最有效和及时的分析，您应该将以下信息传递给统计学家：

1. 原始数据。
2.一个【整洁的数据集】(http://vita.had.co.nz/papers/tidy-data.pdf)
3. 描述 tidy 数据集中每个变量及其值的代码簿。
4. 你曾经从 1 -> 2,3 得到的明确而准确的配方

我们来看看你要传输的数据包的每个部分。


###原始数据

包含您有权访问的最原始形式的数据至关重要。这确保
可以在整个工作流程中维护数据来源。下面是一些例子
原始数据形式：

* 奇怪的[二进制文件](http://en.wikipedia.org/wiki/Binary_file) 你的测量机吐出来的
* 与您签约的公司发送给您的带有 10 个工作表的无格式 Excel 文件
* 您从 [Twitter API](https://twitter.com/twitterapi) 中获取的复杂 [JSON](http://en.wikipedia.org/wiki/JSON) 数据
* 您通过显微镜收集的手工输入数字

如果您符合以下条件，您就知道原始数据的格式正确：

1. 没有对数据运行软件
1.没有修改任何数据值
1. 你没有从数据集中删除任何数据
1.你没有以任何方式汇总数据

如果您对原始数据进行了任何修改，则它不是数据的原始形式。报告修改的数据
因为原始数据是减慢分析过程的一种非常常见的方法，因为分析师通常需要做一个
对您的数据进行取证研究，以找出原始数据看起来很奇怪的原因。 （还想象一下如果新数据到来会发生什么？）

### 整洁的数据集

[Hadley Wickham](http://had.co.nz/) 在 [this paper](http://vita.had.co.nz/papers/tidy-data. pdf)
和 [此视频](http://vimeo.com/33727555)。虽然论文和视频都使用 [R](http://www.r-project.org/) 描述了整洁的数据，但原则
更普遍适用：

1. 你测量的每个变量都应该在一列中
1. 该变量的每个不同观察值都应该在不同的行中
1. 每种“种类”的变量应该有一个表
1.如果你有多个表，他们应该在表中包含一列，允许它们连接或合并

虽然这些是硬性规定，但还有许多其他事情可以使您的数据集更容易
处理。首先是在每个数据表/电子表格的顶部包含一行，其中包含完整的行名称。
因此，如果您在诊断时测量了患者的年龄，则该列的标题将改为“AgeAtDiagnosis”
诸如“ADx”或其他人可能难以理解的其他缩写。


这是基因组学如何运作的一个例子。假设您收集了 20 个人的基因表达测量值
[RNA 测序](http://en.wikipedia.org/wiki/RNA-Seq)。您还收集了人口统计和临床信息
关于患者，包括他们的年龄、治疗和诊断。您将拥有一张包含临床/人口统计数据的表格/电子表格
信息。它将有四列（患者 ID、年龄、治疗、诊断）和 21 行（一行带有变量名称，然后一行
每位患者）。您还将拥有一份用于汇总基因组数据的电子表格。通常这种类型的数据
在每个外显子的计数水平上总结。假设您有 100,000 个外显子，那么您将有一个
表格/电子表格有 21 行（基因名称一行，每位患者一行）和 100,001 列（患者一行）
ids 和每种数据类型的一行）。

如果您在 Excel 中与协作者共享您的数据，则每个表格中的整洁数据应位于一个 Excel 文件中。他们
不应有多个工作表，不应将宏应用于数据，也不应突出显示列/单元格。
或者在 [CSV](http://en.wikipedia.org/wiki/Comma-separated_values) 或 [TAB-delimited](http://en.wikipedia.org/wiki/Tab-separated_values) 文本中共享数据文件。 （但请注意，将 CSV 文件读入 Excel 有时会导致无法重现的日期和时间变量处理。）
### The code book

For almost any data set, the measurements you calculate will need to be described in more detail than you can or should sneak
into the spreadsheet. The code book contains this information. At minimum it should contain:

1. Information about the variables (including units!) in the data set not contained in the tidy data 
1. Information about the summary choices you made
1. Information about the experimental study design you used

In our genomics example, the analyst would want to know what the unit of measurement for each
clinical/demographic variable is (age in years, treatment by name/dose, level of diagnosis and how heterogeneous). They 
would also want to know how you picked the exons you used for summarizing the genomic data (UCSC/Ensembl, etc.). They
would also want to know any other information about how you did the data collection/study design. For example,
are these the first 20 patients that walked into the clinic? Are they 20 highly selected patients by some characteristic
like age? Are they randomized to treatments? 

A common format for this document is a Word file. There should be a section called "Study design" that has a thorough
description of how you collected the data. There is a section called "Code book" that describes each variable and its
units. 
### 密码本

对于几乎任何数据集，您计算的测量值都需要比您可以或应该偷偷摸摸地更详细地描述
进入电子表格。密码本包含此信息。它至少应包含：

1. 数据集中未包含在整洁数据中的变量信息（包括单位！）
1.关于您所做的总结选择的信息
1.关于您使用的实验研究设计的信息

在我们的基因组学示例中，分析师想知道每个的测量单位是什么
临床/人口统计学变量是（年龄、名称/剂量、诊断水平和异质性）。他们
还想知道您如何挑选用于总结基因组数据的外显子（UCSC/Ensembl 等）。他们
还想知道有关您如何进行数据收集/研究设计的任何其他信息。例如，
这些是走进诊所的前 20 名患者吗？他们是 20 位经过某些特征精心挑选的患者吗？
比如年龄？他们是否随机接受治疗？

此文档的常见格式是 Word 文件。应该有一个名为“研究设计”的部分，其中有一个完整的
您如何收集数据的描述。有一个名为“代码手册”的部分描述了每个变量及其
单位。
### How to code variables

When you put variables into a spreadsheet there are several main categories you will run into depending on their [data type](http://en.wikipedia.org/wiki/Statistical_data_type):

1. Continuous
1. Ordinal
1. Categorical
1. Missing 
1. Censored

Continuous variables are anything measured on a quantitative scale that could be any fractional number. An example
would be something like weight measured in kg. [Ordinal data](http://en.wikipedia.org/wiki/Ordinal_data) are data that have a fixed, small (< 100) number of levels but are ordered. 
This could be for example survey responses where the choices are: poor, fair, good. [Categorical data](http://en.wikipedia.org/wiki/Categorical_variable) are data where there
are multiple categories, but they aren't ordered. One example would be sex: male or female. This coding is attractive because it is self-documenting.  [Missing data](http://en.wikipedia.org/wiki/Missing_data) are data
that are unobserved and you don't know the mechanism. You should code missing values as `NA`. [Censored data](http://en.wikipedia.org/wiki/Censoring_\(statistics\)) are data
where you know the missingness mechanism on some level. Common examples are a measurement being below a detection limit
or a patient being lost to follow-up. They should also be coded as `NA` when you don't have the data. But you should
also add a new column to your tidy data called, "VariableNameCensored" which should have values of `TRUE` if censored 
and `FALSE` if not. In the code book you should explain why those values are missing. It is absolutely critical to report
to the analyst if there is a reason you know about that some of the data are missing. You should also not [impute](http://en.wikipedia.org/wiki/Imputation_\(statistics\))/make up/
throw away missing observations.

In general, try to avoid coding categorical or ordinal variables as numbers. When you enter the value for sex in the tidy
data, it should be "male" or "female". The ordinal values in the data set should be "poor", "fair", and "good" not 1, 2 ,3.
This will avoid potential mixups about which direction effects go and will help identify coding errors. 

Always encode every piece of information about your observations using text. For example, if you are storing data in Excel and use a form of colored text or cell background formatting to indicate information about an observation ("red variable entries were observed in experiment 1.") then this information will not be exported (and will be lost!) when the data is exported as raw text.  Every piece of data should be encoded as actual text that can be exported.  

### The instruction list/script

You may have heard this before, but [reproducibility is a big deal in computational science](http://www.sciencemag.org/content/334/6060/1226).
That means, when you submit your paper, the reviewers and the rest of the world should be able to exactly replicate
the analyses from raw data all the way to final results. If you are trying to be efficient, you will likely perform
some summarization/data analysis steps before the data can be considered tidy. 

The ideal thing for you to do when performing summarization is to create a computer script (in `R`, `Python`, or something else) 
that takes the raw data as input and produces the tidy data you are sharing as output. You can try running your script
a couple of times and see if the code produces the same output. 

In many cases, the person who collected the data has incentive to make it tidy for a statistician to speed the process
of collaboration. They may not know how to code in a scripting language. In that case, what you should provide the statistician
is something called [pseudocode](http://en.wikipedia.org/wiki/Pseudocode). It should look something like:

1. Step 1 - take the raw file, run version 3.1.2 of summarize software with parameters a=1, b=2, c=3
1. Step 2 - run the software separately for each sample
1. Step 3 - take column three of outputfile.out for each sample and that is the corresponding row in the output data set

You should also include information about which system (Mac/Windows/Linux) you used the software on and whether you 
tried it more than once to confirm it gave the same results. Ideally, you will run this by a fellow student/labmate
to confirm that they can obtain the same output file you did. 

###如何编码变量

当您将变量放入电子表格时，您会遇到几个主要类别，具体取决于它们的 [数据类型](http://en.wikipedia.org/wiki/Statistical_data_type)：

1. 连续
1.序数
1. 分类
1. 失踪
1. 审查

连续变量是在定量尺度上测量的任何东西，可以是任何小数。一个例子
类似于以公斤为单位的体重。 [序数数据](http://en.wikipedia.org/wiki/Ordinal_data) 是具有固定的小 (< 100) 级数但有序的数据。
例如，这可以是选择为：差、一般、好的调查回复。 [分类数据](http://en.wikipedia.org/wiki/Categorical_variable) 是存在
是多个类别，但它们没有排序。一个例子是性别：男性或女性。这种编码很有吸引力，因为它是自我记录的。 [缺失数据](http://en.wikipedia.org/wiki/Missing_data) 是数据
那些未被观察到的，你不知道机制。您应该将缺失值编码为“NA”。 [截尾数据](http://en.wikipedia.org/wiki/Censoring_\(statistics\)) 是数据
你知道某种程度的缺失机制。常见的例子是低于检测限的测量
或失去随访的患者。当您没有数据时，它们也应编码为“NA”。但是你应该
还向您的整洁数据添加一个名为“VariableNameCensored”的新列，如果被审查，该列的值应为“TRUE”
如果不是，则为“假”。在代码手册中，您应该解释为什么缺少这些值。报告是绝对重要的
如果您知道某些数据丢失的原因，请告知分析师。你也不应该 [impute](http://en.wikipedia.org/wiki/Imputation_\(statistics\))/make up/
丢弃丢失的观察结果。

通常，尽量避免将分类或有序变量编码为数字。当您在 tidy 中输入性的值时
数据，应该是“男”或“女”。数据集中的序数值应该是“差”、“一般”和“好”，而不是 1、2、3。
这将避免关于哪个方向效应发生的潜在混淆，并有助于识别编码错误。

始终使用文本对有关您的观察的每条信息进行编码。例如，如果您在 Excel 中存储数据并使用彩色文本形式或单元格背景格式来指示有关观察的信息（“在实验 1 中观察到红色变量条目。”），则不会导出此信息（并且将丢失！）当数据导出为原始文本时。每条数据都应编码为可以导出的实际文本。

###指令列表/脚本

您以前可能听说过这个，但是 [再现性在计算科学中很重要](http://www.sciencemag.org/content/334/6060/1226)。
这意味着，当您提交论文时，审稿人和世界其他地方应该能够准确复制
从原始数据到最终结果的分析。如果你想提高效率，你很可能会执行
在数据可以被认为是整洁之前的一些总结/数据分析步骤。

执行摘要时最理想的做法是创建一个计算机脚本（在“R”、“Python”或其他语言中）
它将原始数据作为输入并生成您正在共享的整洁数据作为输出。您可以尝试运行您的脚本
几次，看看代码是否产生相同的输出。

在许多情况下，收集数据的人有动力让统计人员将数据整理好以加快流程
的合作。他们可能不知道如何用脚本语言编写代码。在这种情况下，您应该向统计学家提供什么
是一种叫做 [伪代码](http://en.wikipedia.org/wiki/Pseudocode) 的东西。它应该看起来像：

1. 步骤 1 - 获取原始文件，运行 3.1.2 版本的汇总软件，参数 a=1, b=2, c=3
1. 步骤 2 - 为每个样本单独运行软件
1. 步骤 3 - 为每个样本取 outputfile.out 的第三列，即输出数据集中的对应行

您还应该包括有关您在哪个系统 (Mac/Windows/Linux) 上使用该软件以及您是否
多次尝试以确认它给出了相同的结果。理想情况下，您将由同学/实验室伙伴运行此程序
确认他们可以获得与您相同的输出文件。


What you should expect from the analyst
====================

When you turn over a properly tidied data set it dramatically decreases the workload on the statistician. So hopefully
they will get back to you much sooner. But most careful statisticians will check your recipe, ask questions about
steps you performed, and try to confirm that they can obtain the same tidy data that you did with, at minimum, spot
checks.

You should then expect from the statistician:

1. An analysis script that performs each of the analyses (not just instructions)
1. The exact computer code they used to run the analysis
1. All output files/figures they generated. 

This is the information you will use in the supplement to establish reproducibility and precision of your results. Each
of the steps in the analysis should be clearly explained and you should ask questions when you don't understand
what the analyst did. It is the responsibility of both the statistician and the scientist to understand the statistical
analysis. You may not be able to perform the exact analyses without the statistician's code, but you should be able
to explain why the statistician performed each step to a labmate/your principal investigator. 
您应该从分析师那里得到什么
====================

当您翻转经过适当整理的数据集时，它会显着减少统计人员的工作量。所以希望
他们会很快回复你。但大多数细心的统计学家会检查你的食谱，询问有关
您执行的步骤，并尝试确认他们可以获得与您所做的相同的整洁数据，至少，现场
检查。

然后你应该期待统计学家：

1. 执行每个分析的分析脚本（不仅仅是指令）
1. 他们用来运行分析的确切计算机代码
1. 他们生成的所有输出文件/图形。

这是您将在补充材料中用于确定结果的重现性和精确度的信息。每个
分析中的步骤应该清楚地解释，当你不明白的时候你应该提出问题
分析师做了什么。统计学家和科学家都有责任理解统计
分析。如果没有统计学家的代码，您可能无法执行准确的分析，但您应该能够
解释为什么统计学家向实验室伙伴/您的主要研究人员执行每个步骤。

Contributors
====================

* [Jeff Leek](http://biostat.jhsph.edu/~jleek/) - Wrote the initial version.
* [L. Collado-Torres](http://bit.ly/LColladoTorres) - Fixed typos, added links.
* [Nick Reich](http://people.umass.edu/nick/) - Added tips on storing data as text.
* [Nick Horton](https://www.amherst.edu/people/facstaff/nhorton) - Minor wording suggestions.

贡献者
====================

* [Jeff Leek](http://biostat.jhsph.edu/~jleek/) - 编写了初始版本。
* [L. Collado-Torres](http://bit.ly/LColladoTorres) - 修正错别字，添加链接。
* [Nick Reich](http://people.umass.edu/nick/) - 添加了将数据存储为文本的提示。
* [尼克霍顿](https://www.amherst.edu/people/facstaff/nhorton) - 次要措辞建议。
